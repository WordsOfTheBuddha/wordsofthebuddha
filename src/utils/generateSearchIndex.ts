#!/usr/bin/env node

// Build-time generator for a client-usable search dataset (English only).
// Outputs:
// - src/data/searchIndex.ts (typed module for direct imports)

import { readFile, writeFile, mkdir } from "node:fs/promises";
import path from "node:path";
import { fileURLToPath } from "node:url";
import { glob } from "glob";
import matter from "gray-matter";


interface SearchDoc {
  slug: string;
  title: string;
  description?: string;
  content: string;
}

function stripMarkdown(md: string): string {
  try {
    let s = md || "";
    // remove code fences
    s = s.replace(/```[\s\S]*?```/g, " ");
    // images ![alt](url)
    s = s.replace(/!\[[^\]]*\]\([^)]*\)/g, " ");
    // links [text](url) -> text
    s = s.replace(/\[([^\]]+)\]\([^)]*\)/g, "$1");
    // inline bold/italic/code/strikethrough
    s = s.replace(/[*_`~]+/g, "");
    // headings, blockquotes, lists markup
    s = s.replace(/^\s{0,3}[#>\-+*]\s+/gm, "");
    // MDX/tooltip like |text::tooltip|
    s = s.replace(/\|([^|:]+)::([^|]+)\|/g, "$1 $2");
    // collapse whitespace
    s = s.replace(/\s+/g, " ").trim();
    return s;
  } catch {
    return md || "";
  }
}

async function main() {
  const start = Date.now();
  const __dirname = path.dirname(fileURLToPath(import.meta.url));
  const repoRoot = path.resolve(__dirname, "../..");
  const contentRoot = path.join(repoRoot, "src", "content", "en");
  const dataDir = path.join(repoRoot, "src", "data");
  const tsOutFile = path.join(dataDir, "searchIndex.ts");

  const patterns = ["**/*.mdx"]; // English content only
  const files = (
    await Promise.all(patterns.map((p) => glob(path.join(contentRoot, p))))
  ).flat();

  console.log(`search-index: found ${files.length} English content files under ${path.relative(repoRoot, contentRoot)}`);

  const docs: SearchDoc[] = [];

  for (const file of files) {
    try {
      const raw = await readFile(file, "utf8");
      const parsed = matter(raw);
      const data = parsed.data as Record<string, any>;
      const body = parsed.content || "";

      const slug = (data.slug as string) || path.basename(file).replace(/\.[^.]+$/, "");
      const title = (data.title as string) || slug;
      const description = (data.description as string) || undefined;
      const content = stripMarkdown(body);

      // Skip empty content entries to keep size lower
      if (!slug) continue;
      docs.push({ slug, title, description, content });
    } catch (e) {
      console.warn("[search-index] Skipping file due to error:", file, e);
    }
  }

  await mkdir(dataDir, { recursive: true });
  console.log(`search-index: writing to ${tsOutFile}`);
  const json = JSON.stringify(docs);

  // Also emit a typed TS module for direct imports in server/client code
  const tsModule = `// Auto-generated by generateSearchIndex.ts. Do not edit.
// deno-lint-ignore-file
// eslint-disable
export type SearchDoc = { slug: string; title: string; description?: string; content: string };
const searchIndex: SearchDoc[] = ${json} as const;
export default searchIndex;
`;
  await writeFile(tsOutFile, tsModule, "utf8");

  const bytes = Buffer.byteLength(json, "utf8");
  const kb = bytes / 1024;
  const ms = Date.now() - start;
  console.log(`search-index: wrote ${docs.length} docs to TS module (${kb.toFixed(1)} KB) in ${ms}ms`);
}

main().catch((err) => {
  console.error("search-index generation failed:", err);
  process.exit(1);
});

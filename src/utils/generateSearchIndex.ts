#!/usr/bin/env node

// Build-time generator for a client-usable search dataset (English only).
// Outputs:
// - src/data/searchIndex.ts (typed module for direct imports)

import { readFile, writeFile, mkdir } from "node:fs/promises";
import path from "node:path";
import { fileURLToPath } from "node:url";
import { glob } from "glob";
import matter from "gray-matter";

interface SearchDoc {
	slug: string;
	title: string;
	description?: string;
	content?: string;
	maxScore?: number; // Cap the maximum score for this doc (lower = appears lower in results)
	contentSearchable?: boolean; // If false, don't match on content or show content snippets
}

// Pages to exclude from search index (slugs without leading slash)
const BLACKLISTED_SLUGS = new Set([
	"index", // Home page
	"search", // Search page itself
	"signin", // Auth pages
	"register",
	"profile",
	"privacy", // Legal pages
	"public-domain",
	"review-room", // User-specific pages
	"sitemap.xml", // Technical pages
	"robots.txt",
]);

// Virtual pages that aren't MDX but should be searchable
// These are added manually with their metadata
const VIRTUAL_PAGES: SearchDoc[] = [
	{
		slug: "offline",
		title: "Offline Control Center",
		description:
			"Download suttas and collections for offline reading. Manage cached content and install the app.",
	},
];

// Pages where content should not be searched or shown in snippets
// (only title and description are used for matching)
const NO_CONTENT_SEARCH_SLUGS = new Set([
	"in-the-buddhas-words",
	"noble-truths-noble-path",
]);

// Keep content semantically intact but make paragraph boundaries consistent with Astro's body
function normalizeMdContent(md: string): string {
	if (!md) return "";
	// Normalize Windows/Mac newlines to LF and trim trailing spaces
	let s = md.replace(/\r\n?/g, "\n");
	s = s.replace(/[ \t]+$/gm, "");
	// Preserve tooltip syntax |text::tooltip| and blank lines
	return s;
}

const __dirname = path.dirname(fileURLToPath(import.meta.url));
const repoRoot = path.resolve(__dirname, "../..");
const contentRoot = path.join(repoRoot, "src", "content", "en");
const dataDir = path.join(repoRoot, "src", "data");
const tsOutFile = path.join(dataDir, "searchIndex.ts");

/**
 * Parse a single MDX file into a SearchDoc (or null if blacklisted/invalid).
 */
function parseFileToDoc(filePath: string, raw: string): SearchDoc | null {
	const parsed = matter(raw);
	const data = parsed.data as Record<string, any>;
	const body = parsed.content || "";

	const slug =
		(data.slug as string) ||
		path.basename(filePath).replace(/\.[^.]+$/, "");
	const title = (data.title as string) || slug;
	const description = (data.description as string) || undefined;
	const content = normalizeMdContent(body);
	const contentSearchable = !NO_CONTENT_SEARCH_SLUGS.has(slug);

	if (!slug) return null;
	if (BLACKLISTED_SLUGS.has(slug)) return null;

	return {
		slug,
		title,
		description,
		content: contentSearchable ? content : "",
		contentSearchable: contentSearchable ? undefined : false,
	};
}

/** Write the docs array to the TS module file */
async function writeSearchIndex(docs: SearchDoc[]) {
	await mkdir(dataDir, { recursive: true });
	const json = JSON.stringify(docs);
	const tsModule = `// Auto-generated by generateSearchIndex.ts. Do not edit.
// deno-lint-ignore-file
// eslint-disable
export type SearchDoc = { slug: string; title: string; description?: string; content: string; maxScore?: number; contentSearchable?: boolean };
const searchIndex: SearchDoc[] = ${json} as const;
export default searchIndex;
`;
	await writeFile(tsOutFile, tsModule, "utf8");
	return json;
}

/**
 * Incrementally update the search index for a single changed English file.
 * Loads the existing index, replaces/adds the entry for this file, and rewrites.
 */
export async function incrementalSearchIndexUpdate(changedFile: string) {
	const start = Date.now();
	const absPath = path.isAbsolute(changedFile)
		? changedFile
		: path.resolve(changedFile);

	// Read the changed file
	let raw: string;
	try {
		raw = await readFile(absPath, "utf8");
	} catch {
		// File was deleted — remove from index
		const slug = path.basename(absPath).replace(/\.[^.]+$/, "");
		try {
			const existing = await readFile(tsOutFile, "utf8");
			const match = existing.match(
				/\nconst searchIndex: SearchDoc\[\] = (\[.*\]) as const;/,
			);
			if (match) {
				const docs: SearchDoc[] = JSON.parse(match[1]);
				const filtered = docs.filter((d) => d.slug !== slug);
				await writeSearchIndex(filtered);
				console.log(
					`search-index: removed ${slug} (${Date.now() - start}ms)`,
				);
			}
		} catch {
			// No existing index, nothing to remove
		}
		return;
	}

	const newDoc = parseFileToDoc(absPath, raw);
	if (!newDoc) return;

	// Load existing index
	let docs: SearchDoc[] = [];
	try {
		const existing = await readFile(tsOutFile, "utf8");
		const match = existing.match(
			/\nconst searchIndex: SearchDoc\[\] = (\[.*\]) as const;/,
		);
		if (match) {
			docs = JSON.parse(match[1]);
		}
	} catch {
		// No existing index — will create fresh with just this doc
	}

	// Replace existing entry or add new one
	const existingIdx = docs.findIndex((d) => d.slug === newDoc.slug);
	if (existingIdx !== -1) {
		const old = docs[existingIdx];
		// Skip write if only body content changed (avoids Vite re-render).
		// Title/description changes still trigger a write for collection pages.
		if (
			old.slug === newDoc.slug &&
			old.title === newDoc.title &&
			old.description === newDoc.description
		) {
			console.log(
				`search-index: skipped (body-only change) for ${newDoc.slug} (${Date.now() - start}ms)`,
			);
			return;
		}
		docs[existingIdx] = newDoc;
	} else {
		docs.push(newDoc);
	}

	const json = await writeSearchIndex(docs);
	const ms = Date.now() - start;
	console.log(
		`search-index: ${existingIdx !== -1 ? "updated" : "added"} ${newDoc.slug} (${ms}ms)`,
	);
}

export async function fullBuild() {
	const start = Date.now();

	const patterns = ["**/*.mdx"]; // English content only
	const files = (
		await Promise.all(patterns.map((p) => glob(path.join(contentRoot, p))))
	).flat();

	console.log(
		`search-index: found ${
			files.length
		} English content files under ${path.relative(repoRoot, contentRoot)}`,
	);

	const docs: SearchDoc[] = [];

	for (const file of files) {
		try {
			const raw = await readFile(file, "utf8");
			const doc = parseFileToDoc(file, raw);
			if (doc) docs.push(doc);
		} catch (e) {
			console.warn("[search-index] Skipping file due to error:", file, e);
		}
	}

	// Add virtual pages (non-MDX pages that should be searchable)
	for (const virtualPage of VIRTUAL_PAGES) {
		console.log(`search-index: adding virtual page: ${virtualPage.slug}`);
		docs.push(virtualPage);
	}

	const json = await writeSearchIndex(docs);

	const bytes = Buffer.byteLength(json, "utf8");
	const kb = bytes / 1024;
	const ms = Date.now() - start;
	console.log(
		`search-index: wrote ${docs.length} docs to TS module (${kb.toFixed(
			1,
		)} KB) in ${ms}ms`,
	);
}

async function main() {
	// Parse CLI args: --file <path> for incremental update
	const args = process.argv.slice(2);
	const fileIdx = args.indexOf("--file");

	if (fileIdx !== -1 && args[fileIdx + 1]) {
		await incrementalSearchIndexUpdate(args[fileIdx + 1]);
	} else {
		await fullBuild();
	}
}

// Only run when executed directly (not when imported as a module)
const isDirectRun = process.argv[1]?.includes("generateSearchIndex");
if (isDirectRun) {
	main().catch((err) => {
		console.error("search-index generation failed:", err);
		process.exit(1);
	});
}
